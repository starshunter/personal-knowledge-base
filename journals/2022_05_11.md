- # Agenda
	- #+BEGIN_QUERY
	  {:title [:h2 "Scheduled"]
	    :query [:find (pull ?block [* {:block/_parent ...}])
	            :in $ ?start ?next
	            :where
	            (or
	              [?block :block/scheduled ?d])
	            [(>= ?d ?start)]
	            [(<= ?d ?next)]]
	  :inputs [:7d-before :today]
	    :collapsed? false}
	  #+END_QUERY
	- #+BEGIN_QUERY
	  {:title [:h2 "Deadline"]
	    :query [:find (pull ?block [* {:block/_parent ...}])
	            :in $ ?start ?next
	            :where
	              [?block :block/deadline ?d]
	            [(>= ?d ?start)]
	            [(<= ?d ?next)]]
	    :inputs [:7d-before :today]
	    :collapsed? false}
	  #+END_QUERY
# Daily Metrics
	- DONE commit journal
	- CANCELED workout
	- DONE LeetCode Daily Problem
	  :LOGBOOK:
	  CLOCK: [2022-05-11 Wed 08:22:12]--[2022-05-11 Wed 08:51:49] =>  00:29:37
	  :END:
	- DONE Learn Daily JavaScript Question
	  :LOGBOOK:
	  CLOCK: [2022-05-11 Wed 08:51:52]--[2022-05-11 Wed 08:54:51] =>  00:02:59
	  :END:
# Daily Log
	- ## Midnight
		- [[My Family/EP5]]
		  collapsed:: true
			- katsuragi met with toudou, told him he would be moved away from the case
			- he also told him he thought koharu still alive
			- katsuragi met with misaki
			- haruto started to transfer ransom
			- the kidnapper asked them to transfer the ransom back and forth
			- the kidnapper postponed the transaction to the next day
			- miwa suspected satsuki faked the kidnapping
			- tomoka fainted at school
			- miwa confronted satsuki
			- toudou said he also suspected his wife before
			- satsuki said yutsuki went to supermarket at night because she wanted to earn pocket money
			- katsuragi and his boss went to akutsu's home to apologize
			- katsuragi challenged akutsu, said his success might have something to do with tomoka's kidnapping case
			- katsuragi found out misaki was at the same school as koharu and tomoka when the kidnapping case happened
			- michiru smelled perfume on haruto
			- michiru went back to her parents' home
			- yutsuki's asthma's broke up, the kidnapper move the transaction time forward
			- haruto decided to let toudou go catch the kidnaper
			- michiru called haruto, said she was following kidnapper's car
			- toudou stopped her in time
			- haruto reached the location the kidnapper told them
			- they found yutsuki
			- miwa wanted to stay beside yutsuki and satsuki
			- the kidnapper returned 20% of the ransom
			- the kidnapper said he kidnapped misaki, and told haruto not to tell anyone else except akutsus
		- 02:28 #[[Bed Time]]
	- ## Morning
		- 08:12 #[[Wake Time]]
		- asked [[Fox Wu]] about how okta middleware handle the request after user being redirected to callback path [[Sherlock/Code]]
			- he didn't dig deep into it, the package might take care all the process
		- gather notes for [[PI/Standup]]
			- study [[Sherlock/Code]] frontend, especially about api and server
			- plan to finish the rest
			- [[Fox Wu]] mentioned some possible change in the future, and how would part of the development
		- [[PI/Standup]]
		- [Apache Oozie](https://learning.oreilly.com/library/view/apache-oozie/9781449369910) 
		  id:: 627b25f4-924b-4c28-8e80-6ef0bd62b1f7
		  type:: book
		  tags:: Hadoop, Oozie, Hadoop
		  title:: Apache Oozie
		  collapsed:: true
			- Oozie is designed to run multistage Hadoop jobs as a single job, an Oozie job
			- an Oozie workflow is a multistage Hadoop job
			- an Oozie coordinator schedules workflow executions based on a start-time and a frequency parameter, and it starts the workflow when all the necessary input data becomes available
				- if the input data is not available for a workflow run, the execution of the workflow job will be delayed until the input data becomes available
				- if the daily rawlogs are not available for a few days, the coordinator job keeps track of all missed days, and when the rawlogs for a missing day shows up, the workflow to process the logs for the corresponding date is started
			- an Oozie bundle is a collection of coordinator jobs that can be started, stopped, suspended, and modified as a single job, typically, coordinator jobs in a bundle depend on each other
			- Oozie supports a set of functions that can be used to carry out sophisticated logic for resolving variable values during the execution of the Oozie job
			- the first step in many big data analytic platforms is usually data ingestion from some upstream data source into Hadoop
			- action nodes define jobs, which are the individual units of work that are chained together to make up the Oozie workflow
			- Oozie runs the actual actions through a launcher job, which itself is a Hadoop MapReduce job that runs on the Hadpoop cluster, the launcher is a map-only job that runs only one mapper
			- delegating the client responsibilities to the launcher job makes sure that the execution of that ode will not overload or overwhelm the Oozie server machine
				- this architecture also means that the action code and configuration have to be packaged as a self-contained application and must reside on HDFS for access across the cluster
				- Oozie's XML specification for each action is designed to define and deploy these jobs as self-contained application
			- at the very beginning of the CML is the root element with and `xmlns` and a name attribute specifying the name of the workflow application
			- workflow control nodes define the start and the end of a workflow and they define any control changes in the execution flow
			- when actions don't depend on the result of each other, it is possible to execution actions in parallel using the `<fork>` and `<join>` control nodes to speed up the execution of the workflow
				- all the paths of a `<fork>` node must converge into a `<join>` node, a workflow does not proceed its execution beyond the node until all execution paths from the `<fork>` node reach the `<join>` node
				- the `<path>` elements within the `<fork>` node define the parallel execution paths of the `<fork>` node
			- a `<decision>` node behavior is best described as an `if-then-else-if-else` sequence
			- if any execution path of a workflow reaches a `<kill>` node, Oozie will terminate the workflow immediately, failing all running actions
				- it is worth noting that Oozie will not explicitly kill the currently running MapReduce jobs on the Hadoop cluster that corresponds to those actions, they will be allowed to complete, though the action will be set to `FAILED` and no downstream actions of those jobs in their respective `<fork>-<join>` block will be run
			- when an action completes, its status typically in either `OK` or `ERROR` status depending on whether or not the execution was successful
			- the properties defined in the `<global>` section are available to all actions of the workflow
			- different action nodes in the workflow can include `different job-xml` files
			- inline configuration in the body of the workflow action holds higher priority than the `<global>` section and the `<job-xml>` files
			- Oozie also supports variables like `${YEAR}`, `${MONTH}` and `${DAY}` that you will use often in Oozie coordinators
			- workflow status are: `PREP`, `RUNNING`, `SUCCEEDED`, `KILLED`, `DEAD` and `SUSPENDED`
			- when workflow is `SUSPENDED`, if the workflow was execution a Hadoop job, the Hadoop job will continue running until completion, Hadoop jobs cannot be paused
			- as of now, the Oozie coordinator supports two of the most common triggering mechanisms, namely time and data availability
			- when a coordinator application is submitted to Oozie with all its parameters and configurations, it is called a coordinator job
			- each coordinator action must have a nominal time
			- the second attribute is `xmlns`, which specifies the coordinator namespace used for coordinator XML versioning, the namespace plays a critical role in ensuring backward compatibility of the coordinator
			- as of now, Oozie coordinator only supports launching Oozie workflows and a coordinator application can only include one workflow application
			- the main function of a coordinator job is to create a coordinator action for the specific time instance
			- Oozie keeps the job in `PREP` state until it reaches the start time
			- the final state of a coordinator job depends on the states of all spawned coordinator actions
			- when a coordinator job materializes a coordinator action, Oozie assigns the action to the `WAITING` state
			- if any of the dependent data is still missing after the timeout period, Oozie transitions the action to the `TIMEDOUT` state
			- depending on whether the workflow fails, succeeds, or gets killed, Oozie transitions the state of the coordinator action to `FAILED`, `SUCCEEDED`, or `KILLED`, respectively
			- a coordinator job periodically creates coordinator actions, therefore, if we can regulate this materialization, the ultimate number of outstanding actions can be controlled
				- Oozie provides a user-level control knob called `throttle`, which a user can specify in her coordinator XML, this controls how many maximum coordinator actions can be in the `WAITING` state for a coordinator job at any instant, if no value is specified, the system default value of 12 is used
			- `timeout` enforced how long each coordinator action can be in `WAITING`
			- if there are multiple actions in the `READY` state, Oozie needs to determine which workflow to submit first, this `execution` knob specifies which order Oozie should follow
			- `concurrency` dictates how many coordinator actions of a job can run simultaneously
			- a dataset is a logical entity to represent a set of data produced by any application
			- in Oozie, the dataset produced in regular frequency is called `synchronous` and the dataset produced randomly is known as `asynchronous`
			- there can be more than one dataset in a coordinator
			- `initial-instance` specifies the first time instance of valid data in a dataset
			- `uri-template` specifies the template of the data directory in a dataset
			- these dataset settings might not have any direct association with the timeline defined for the coordinator itself
			- whereas `datasets` declare data items of interest, `<input-events>` describe the actual instances of dependent dataset for the coordinator
			- each `data-in` handles one dataset dependency
			- a `data-in` can include one or more data instances of that dataset, each data instance typically corresponds to a time interval and has a direct association with one directory on HDFS
			- each instance is basically a timestamp that will eventually be used to replace the variables defined in the `<uri-template>` of a dataset definition
			- in an Oozie coordinator, `<output-events>` specifies the data instance produced by a coordinator action
			- there can be only one `<data-out>` section under `output-events`
			- each `data-out` can contain only one instance and multiple instances are note allowed
			- if you want to process the same dependent datasets irrespective of when the job executes, you should use `current()`
			- if you want to process the latest available data at the time of execution irrespective of the nominal time, you should use the `latest()` function
	- ## Afternoon
		-
	- ## Night
		-
# Upcoming
	- #+BEGIN_QUERY
	  {:title [:h2 "Scheduled"]
	    :query [:find (pull ?block [* {:block/_parent ...}])
	            :in $ ?start ?next
	            :where
	              [?block :block/scheduled ?d]
	            [(> ?d ?start)]
	            [(< ?d ?next)]]
	  :result-transform (fn [result]
	                          (sort-by (fn [d]
	                                     (get d :block/scheduled) ) result))    
	  :inputs [:today :7d-after]
	    :collapsed? false}
	  #+END_QUERY
	- #+BEGIN_QUERY
	  {:title [:h2 "Deadline"]
	    :query [:find (pull ?block [* {:block/_parent ...}])
	            :in $ ?start ?next
	            :where
	              [?block :block/deadline ?d]
	            [(> ?d ?start)]
	            [(< ?d ?next)]]
	  :result-transform (fn [result]
	                          (sort-by (fn [d]
	                                     (get d :block/deadline) ) result))    
	  :inputs [:today :7d-after]
	    :collapsed? false}
	  #+END_QUERY
# Someday
	- query-table:: false
	  #+BEGIN_QUERY
	  {:title [:h2 "Tasks"]
	   :query [:find (pull ?b [* {:block/_parent ...}])
	          :where
	          [?b :block/marker ?marker]
	          (not [?b :block/scheduled ?d])
	          (not [?b :block/deadline ?d])
	  (not (page-ref ?b "templates"))
	          [(contains? #{"TODO" "DOING"} ?marker)]]
	  }
	  #+END_QUERY