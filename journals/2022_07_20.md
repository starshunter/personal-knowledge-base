- # Agenda
	- #+BEGIN_QUERY
	  {:title [:h2 "Scheduled"]
	    :query [:find (pull ?block [* {:block/_parent ...}])
	            :in $ ?start ?next
	            :where
	            (or
	              [?block :block/scheduled ?d])
	            [(>= ?d ?start)]
	            [(<= ?d ?next)]]
	  :inputs [:7d-before :today]
	    :collapsed? false}
	  #+END_QUERY
	- #+BEGIN_QUERY
	  {:title [:h2 "Deadline"]
	    :query [:find (pull ?block [* {:block/_parent ...}])
	            :in $ ?start ?next
	            :where
	              [?block :block/deadline ?d]
	            [(>= ?d ?start)]
	            [(<= ?d ?next)]]
	    :inputs [:7d-before :today]
	    :collapsed? false}
	  #+END_QUERY
# Daily Metrics
	- DONE commit journal
	- CANCELED workout
	- DONE LeetCode Daily Problem
	  :LOGBOOK:
	  CLOCK: [2022-07-20 Wed 08:27:29]--[2022-07-20 Wed 08:51:31] =>  00:24:02
	  :END:
	- DONE Learn Daily JavaScript Question
	  :LOGBOOK:
	  CLOCK: [2022-07-20 Wed 08:51:34]--[2022-07-20 Wed 08:55:28] =>  00:03:54
	  :END:
# Daily Log
	- ## Midnight
		- 03:01 #[[Bed Time]]
	- ## Morning
		- 08:14 #[[Wake Time]]
		- gather notes for [[PI/Standup]]
			- for pi dashboard, I finished adding fast type report yesterday, and I also fixed the SLA line missing problem when the vertical axis becomes too large, and for Sherlock, I am running the script with the entire sample input data, and I noticed some requests cannot be processed by SMP or HLFS, I think it's because SMP doesn't support some image format like svg, and some domains block the crawler from crawling their page, so I will run the script again and log the responses to see how many requests are not handled properly
		- [[PI/Standup]]
		- create a [[Pull Request]] for [[PI/Dashboard]]
			- [[Fox Wu]] approved it
			- [[Jack Chiang]] approved it
			- I don't have access to merge it
	- ## Afternoon
		- found out the 550 error from [[HLFS]] stems from [[LLFS]] #Sherlock
			- the domain specifies a `robots.txt` to block the crawler
			- counted the portion of this error
			- logged LLFS response header and `robots.txt`
		- [[HLFS]] cannot crawl 20% of total unique referrer domain #Sherlock
		- [[SMP]] cannot process 10% of total images #Sherlock
			- maybe using base64 to send the images?
		- prepare for tomorrow's [[Sherlock/Meeting]]
		  id:: 62d8296c-e44d-4bfb-9eb8-2df6e162d817
			- so for HLFS, I remembered we talked about for every referrer domain in the input data, I should use HLFS to get all the images on that page, and then go down one level from that page to get more images, but I tried all built in option of HLFS this week, I didn't find a option that could tell HLFS to do this, so right now when I feed a referrer domain to HLFS, it can only return the images on that certain page, so to get the result we have talked about, I will need input data to contain all the url to the pages we want to get images from
			  id:: 62d79c7d-d8ed-42f2-a18d-0b30b4bbdc52
			- for example, this is one of the domain I get in the input data, because I can only provide this url to HLFS, it can only give me images on this specific page, and to get images on other page, I need this specific url in the input data
			- so I found a ticket on jira that has similar use case, it first used HLFS to get the images, then use SMP to score the images, and the first attachment is the input data, and I think this kind of input data is what I need to get the desired result, because it not only list a domain, it also list many pages under that domain, so HLFS is able to go into each page under the domain and get more images, so the third attachment is the output after using HLFS, you can see HLFS is able to get several images from each pages, and currently, because I only get one url for a domain, the result is far less than what it should be
			- I haven't dig into this yet, but I want to mention it to let you guys know, so I'm sending the referrer domain in the new sample data to HLFS to get images, and I noticed some domains block HLFS from accessing their content, so basically they setup a rule to tell HLFS, and any other similar crawler not to crawl their pages, and out of four hundreds unique domains from the input data, I count 53 domains that have set up this rule, so HLFS cannot give me the images on these 53 domains
			- for other crawler, I believe there are ways to bypass this rule, but I'm not sure if HLFS offers this option, and even if it does, bypass the rule without prior notice may cause some problems, I know nothing about legal problem, but they could ban HLFS's IP if they find there's unauthorized crawler crawling their pages, so for these domains, I think it is best to notify their owner before using HLFS, and ask them to allow HLFS to access their pages, or just switch to manual review to find adult content for these domains
	- ## Night
		- studied if bypassing `robots.txt` will cause problem #Sherlock
			- probably not legally, but the owner could ban [[HLFS]]'s IP address
# Upcoming
	- #+BEGIN_QUERY
	  {:title [:h2 "Scheduled"]
	    :query [:find (pull ?block [* {:block/_parent ...}])
	            :in $ ?start ?next
	            :where
	              [?block :block/scheduled ?d]
	            [(> ?d ?start)]
	            [(< ?d ?next)]]
	  :result-transform (fn [result]
	                          (sort-by (fn [d]
	                                     (get d :block/scheduled) ) result))    
	  :inputs [:today :7d-after]
	    :collapsed? false}
	  #+END_QUERY
	- #+BEGIN_QUERY
	  {:title [:h2 "Deadline"]
	    :query [:find (pull ?block [* {:block/_parent ...}])
	            :in $ ?start ?next
	            :where
	              [?block :block/deadline ?d]
	            [(> ?d ?start)]
	            [(< ?d ?next)]]
	  :result-transform (fn [result]
	                          (sort-by (fn [d]
	                                     (get d :block/deadline) ) result))    
	  :inputs [:today :7d-after]
	    :collapsed? false}
	  #+END_QUERY
# Someday
	- query-table:: false
	  #+BEGIN_QUERY
	  {:title [:h2 "Tasks"]
	   :query [:find (pull ?b [* {:block/_parent ...}])
	          :where
	          [?b :block/marker ?marker]
	          (not [?b :block/scheduled ?d])
	          (not [?b :block/deadline ?d])
	  (not (page-ref ?b "templates"))
	          [(contains? #{"TODO" "DOING"} ?marker)]]
	  }
	  #+END_QUERY